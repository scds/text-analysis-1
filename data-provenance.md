---
layout: default
title: Data Provenance
parent: Lesson
nav_order: 2
---
## Data Provenance

Anyone who has worked with archival materials before (which we may assume encompasses some of the lesson's intended audience!) is likely familiar with the concept of provenance, referring to the origins of something (e.g. a collection or archival fonds). Data provenance, then, traces the origins and lineage of your dataset throughout its lifecycle including:
* where you got the data from
* what transformations your performed upon the data
* what steps you took during your analysis of the data and the thinking behind it

Documenting the provenance of your data helps to establish the accuracy, reliability and [authenticity of your results](https://www.ands.org.au/working-with-data/publishing-and-reusing-data/data-provenance) and also allows you to trace back and correct errors when you discover them later in the textual data analysis workflow.

If you are using a tool like OpenRefine for pre-processing, some of the documentation can be automated; as we will see in "[Correcting OCR Errors with OpenRefine](ocr-correction.html)," OpenRefine keeps track of every operation you perform which can be exported in JSON format.

A more detailed discussion of data provenance is outside of the scope of the current lesson, but if you wish to pursue the topic further you can explore Paolo Missier's "[Provenance Standards](http://homepages.cs.ncl.ac.uk/paolo.missier/doc/Provenance-standards.pdf)" and the [Provenance Tools page](https://projects.iq.harvard.edu/provenance-at-harvard/tools) created by the Provenance@Harvard group at Harvard University.

Next -> [Correcting OCR Errors with OpenRefine](ocr-correction.html)
