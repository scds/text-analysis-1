---
layout: default
title: An Overview of the Textual Analysis Workflow
parent: Lesson
nav_order: 1
---

When performing textual analysis through computational means, we are working with texts as data. Thomas Padilla explains, "if the notion of a single digitized text is shifted from a surrogate of a bound paper object to consider the possibility latent in a form that is computationally processable at the level of thousands or even  millions of texts, a move is made toward meaning making that engages affordances unique to data" (Padilla, 1). That is, putting text into a form (data) amenable to the various tasks and operations we can perform with a computer allows for new approaches to understanding the underlying texts.

Once text is transformed into data, or datafied, it requires as in any computer. The graphic below is 

\[graphic of workflow]

Acquisition (Input): the text corpus is assembled in a datafied form that can be operated upon computationally. This may involve running optical character recognition (OCR) on scanned physical texts or downloading from an application programming interface (API).

Initial data analysis (IDA): 

Pre-processing:

Exporatory data analysis (EDA):

(Output): 

Of course, the various stages of the workflow may overlap and. When , we may observe characteristics of the data that are sought in the exploratory data analysis phase.

Each stage of the workflow that involves a data transformation (Acquire, Pre-Processing, Output; any data analysis should be performed on a copy of the original dataset to avoid unintentionally transforming the data) holds the potential to introduce errors that often have consequences for later stages of the workflow, which may not become evident until you actually arrive at that stage. It is therefore important to document the transformations you perform so that errors can be traced back and corrected. This documentation also contributes to the record of  creation, or its **data provenance**.


Next --> Data Provenance
